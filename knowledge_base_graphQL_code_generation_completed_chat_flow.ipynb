{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKyijkWb1g+kHyCnYD/FWI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omkar806/on-device-experiments/blob/main/knowledge_base_graphQL_code_generation_completed_chat_flow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run the below cell to install all the libraries"
      ],
      "metadata": {
        "id": "Akm2iYKs7gx9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kr9UWmYOI2l",
        "outputId": "a73282e7-1f99-470e-d1ab-5f720d3064bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.1/324.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.4/124.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain openai langchain_openai langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import all the neccessary libraries using the below cell"
      ],
      "metadata": {
        "id": "5kBuWSjB7rmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.callbacks import get_openai_callback\n",
        "import openai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from collections import deque\n",
        "import requests\n",
        "import json"
      ],
      "metadata": {
        "id": "alHV5SN8QbqH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Defining a Pydantic Model to extract the exact information for Each key and Validate the json output given by the LLM."
      ],
      "metadata": {
        "id": "g048lk_m75jN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialising the OpenAI model , Please provide your valid OPENAI_API_KEY in the cell below in the first line"
      ],
      "metadata": {
        "id": "nouQlfyZ8lfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY=userdata.get(\"OPENAI_API_KEY\")\n",
        "model_name = \"gpt-3.5-turbo-instruct\"\n",
        "temperature = 0.0\n",
        "model = OpenAI(openai_api_key=API_KEY,model_name=model_name, temperature=temperature, max_tokens=800)"
      ],
      "metadata": {
        "id": "fmxkPt7BQtyB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function responsible for taking the user query processing it and Validating it with Pydantic model keys and printing the Json output"
      ],
      "metadata": {
        "id": "-HAR29u287Gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graphQL_template = \"\"\"\n",
        "\n",
        "The response should understand a user question and return a graphql query according to it.\n",
        "The Supabase table under consideration is named as \"receipt_radar_insights\" and it has the following column_names:\n",
        "purchaseCategories,competitorReceipts,shoppingDates,spendingCapacity\n",
        "The response must ONLY contain the code snippet and NOTHING else.\n",
        "The response must be one single line which contains only the query and must not be assigned to a variable.\n",
        "Below are some examples for you to what response you have to exactly generate.\n",
        "Example 1:\n",
        "Question: what's is previuous month budget ?\n",
        "Response:\n",
        "{\n",
        "  receipt_radar_insightsCollection\n",
        "{\n",
        "  edges\n",
        "{\n",
        "  node\n",
        "{shoppingDates spendingCapacity}\n",
        "\n",
        "}\n",
        "  }\n",
        "    }\n",
        "\n",
        "Example 2:\n",
        "Question: What did I spend on last month?\n",
        "Response:\n",
        "{\n",
        "  receipt_radar_insightsCollection\n",
        "{\n",
        "  edges\n",
        "{\n",
        "  node\n",
        "{purchaseCategories}\n",
        "\n",
        "}\n",
        "  }\n",
        "    }\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zl3cIg7Jlg3H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_ref = userdata.get(\"PROJECT_REF\")\n",
        "api_key = userdata.get(\"SUPABASE_API_KEY\")\n",
        "url = f\"https://{project_ref}.supabase.co/graphql/v1\"\n",
        "headers = {\n",
        "    \"apiKey\": api_key,\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "query = \"\"\"{\n",
        "  receipt_radar_insightsCollection\n",
        "{\n",
        "  edges\n",
        "{\n",
        "  node\n",
        "{spendingCapacity}\n",
        "\n",
        "}\n",
        "  }\n",
        "}\"\"\"\n",
        "\n",
        "payload = {\n",
        "    \"query\": query,\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, json=payload)\n",
        "\n",
        "print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFb9pM9_FW9b",
        "outputId": "dd311c4b-4da0-4f63-adb4-1f3853cdf014"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'data': {'receipt_radar_insightsCollection': {'edges': [{'node': {'spendingCapacity': 8686.375}}]}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_data_supabase(supabase_query):\n",
        "  project_ref = os.getenv(\"PROJECT_REF\")\n",
        "  api_key = os.getenv(\"SUPABASE_API_KEY\")\n",
        "\n",
        "  # Define the URL and headers\n",
        "  url = f\"https://{project_ref}.supabase.co/graphql/v1\"\n",
        "  headers = {\n",
        "      \"apiKey\": api_key,\n",
        "      \"Content-Type\": \"application/json\"\n",
        "  }\n",
        "\n",
        "  # Define the GraphQL query\n",
        "  query = f\"{supabase_query}\"\n",
        "\n",
        "  print(query)\n",
        "  print(type(query))\n",
        "  # Define the payload\n",
        "  payload = {\n",
        "      \"query\": query,\n",
        "\n",
        "  }\n",
        "\n",
        "  # Make the POST request\n",
        "  response = requests.post(url, headers=headers, json=payload)\n",
        "\n",
        "  # Print the response\n",
        "  print(response.json())\n",
        "  return response.json()"
      ],
      "metadata": {
        "id": "KNEbDBhcsuFJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bot_code_generation_response(user_query):\n",
        "  try:\n",
        "    prompt = PromptTemplate(\n",
        "      template=\"Answer the user query based on the below examples and template.\\n{graphQL_template}\\n User Query :\\n {query} \\n Generate a graphql query based on above question and examples given.\",\n",
        "      input_variables=[\"query\"],\n",
        "      partial_variables={\"graphQL_template\": graphQL_template},\n",
        "    )\n",
        "    input = prompt.format_prompt(query=str(user_query))\n",
        "    # print(input.to_string())\n",
        "    with get_openai_callback() as cb:\n",
        "      try:\n",
        "        result = model.invoke(input.to_string())\n",
        "        response=get_user_data_supabase(result)\n",
        "        print(\"printing response in bot_code_generation\")\n",
        "        print(\"printing result\",result)\n",
        "        return response\n",
        "        # return result\n",
        "      except Exception as e:\n",
        "        print(f\"Error {e}\")\n",
        "        return e\n",
        "  except Exception as e:\n",
        "    print(f\"Error : {e}\")\n",
        "    return e\n"
      ],
      "metadata": {
        "id": "SvbWwYH37c6e"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Below cell is the response of the bot on the user_query"
      ],
      "metadata": {
        "id": "NRzzkWhA9Il4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining a prompt template for\n",
        "user_query_response_template = \"\"\"\n",
        "Answer the user query based on the context provided.\\n\n",
        "\n",
        "User query : {query} \\n\n",
        "\n",
        "context : {context}\\n\n",
        "\n",
        "Response History : {response_history}\\n\n",
        "Above is the context and response history provided ,context is a json analyse it and answer the user query above based on this context and the user_history ,if the question is related to any historical reponses.\n",
        "\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "UPbNX-bE7yBd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bot_response_supabase(user_query,user_data,response_history):\n",
        "  try:\n",
        "    prompt = PromptTemplate(\n",
        "      template=user_query_response_template,\n",
        "      input_variables=[\"query\"],\n",
        "      partial_variables={\"context\": user_data , \"response_history\":str(list(response_history))},\n",
        "    )\n",
        "    input = prompt.format_prompt(query=str(user_query))\n",
        "    # print(input.to_string())\n",
        "    with get_openai_callback() as cb:\n",
        "      try:\n",
        "        result = model.invoke(input.to_string())\n",
        "        print(\"printing result\")\n",
        "        print(result)\n",
        "      except Exception as e:\n",
        "        print(f\"Error {e}\")\n",
        "\n",
        "      try:\n",
        "        response_history.append(result)\n",
        "      except Exception as e:\n",
        "        response_history.append(\"No valid previous response\")\n",
        "        print(f\"Error {e}\")\n",
        "  except Exception as e:\n",
        "    print(f\"Error : {e}\")\n"
      ],
      "metadata": {
        "id": "WjCavZMF64QB"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message_history = deque(maxlen=4)\n",
        "while True:\n",
        "  user_query = input(\"User: \")\n",
        "  user_query = user_query\n",
        "  if user_query == \"exit\":\n",
        "    break\n",
        "  user_data=json.dumps(bot_code_generation_response(user_query))\n",
        "  #generating a proper response based on user query\n",
        "  bot_response_supabase(user_query,user_data,message_history)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "id": "PIuWSz3gm7oS",
        "outputId": "852db48e-77f3-43d8-d93d-b86fc4d518b9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: what is my budget ? \n",
            "\n",
            "\n",
            "{\n",
            "  receipt_radar_insightsCollection\n",
            "{\n",
            "  edges\n",
            "{\n",
            "  node\n",
            "{spendingCapacity}\n",
            "\n",
            "}\n",
            "  }\n",
            "}\n",
            "<class 'str'>\n",
            "Error HTTPSConnectionPool(host='none.supabase.co', port=443): Max retries exceeded with url: /graphql/v1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f4eac903a00>: Failed to resolve 'none.supabase.co' ([Errno -2] Name or service not known)\"))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Object of type ConnectionError is not JSON serializable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-3f6cb6ce8a2c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0muser_query\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0muser_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbot_code_generation_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;31m#generating a proper response based on user query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mbot_response_supabase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_query\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmessage_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
            "\u001b[0;32m/usr/lib/python3.10/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type ConnectionError is not JSON serializable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9RsvlL08sn0q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}